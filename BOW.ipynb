{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BOW.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNgHJmd3m4Vj7plZAIq1U7h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityaiitg/Learning-NLP/blob/main/BOW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"Joe waited for the train\", \"The train was late\", \"Mary and Samantha took the bus\",\n",
        " \"I looked for Mary and Samantha at the bus station\",\n",
        "\"Mary and Samantha arrived at the bus station early but waited until noon for the bus\"]"
      ],
      "metadata": {
        "id": "XhpCHkBjUKmL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def word_extraction(sentence):\n",
        "  ignore = ['a', \"the\", \"is\"] \n",
        "  words = re.sub(\"[^\\w]\", \" \",  sentence).split()\n",
        "  cleaned_text = [w.lower() for w in words if w not in ignore]\n",
        "  return cleaned_text"
      ],
      "metadata": {
        "id": "FGx3FcfSVCWx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy"
      ],
      "metadata": {
        "id": "uBGt7Wa4VPw3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentences):\n",
        "  words = []\n",
        "  for sentence in sentences:\n",
        "    w = word_extraction(sentence)\n",
        "    words.extend(w)            \n",
        "    words = sorted(list(set(words)))    \n",
        "  return words"
      ],
      "metadata": {
        "id": "r-hpw7gTVbDM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize(text)"
      ],
      "metadata": {
        "id": "UakdEykJVkdN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_bow(allsentences):        \n",
        "  vocab = tokenize(allsentences)    \n",
        "  print(\"Word List for Document \\n{0} \\n\".format(vocab));\n",
        "  for sentence in allsentences:\n",
        "    words = word_extraction(sentence)\n",
        "    bag_vector = numpy.zeros(len(vocab))\n",
        "    for w in words:\n",
        "      for i,word in enumerate(vocab):\n",
        "        if word == w:\n",
        "          bag_vector[i] += 1                            \n",
        "    print(\"{0}\\n{1}\\n\".format(sentence,numpy.array(bag_vector)))"
      ],
      "metadata": {
        "id": "kvUEBxi1Wv56"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_bow(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sWOGDlFW7-L",
        "outputId": "867100f8-3eff-4835-e551-b3aebd20836f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word List for Document \n",
            "['and', 'arrived', 'at', 'bus', 'but', 'early', 'for', 'i', 'joe', 'late', 'looked', 'mary', 'noon', 'samantha', 'station', 'the', 'took', 'train', 'until', 'waited', 'was'] \n",
            "\n",
            "Joe waited for the train\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
            "\n",
            "The train was late\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]\n",
            "\n",
            "Mary and Samantha took the bus\n",
            "[1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            "\n",
            "I looked for Mary and Samantha at the bus station\n",
            "[1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "\n",
            "Mary and Samantha arrived at the bus station early but waited until noon for the bus\n",
            "[1. 1. 1. 2. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0.]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Direct Function"
      ],
      "metadata": {
        "id": "2s8V_Q2OZi5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(text)\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JrG4J4oXU_t",
        "outputId": "cca10659-11ba-413e-8392-75f44aa853d0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1]\n",
            " [1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0]\n",
            " [1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0]\n",
            " [1 1 1 2 1 1 1 0 0 0 1 1 1 1 2 0 0 1 1 0]]\n"
          ]
        }
      ]
    }
  ]
}